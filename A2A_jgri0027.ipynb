{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adda5bfa-1f9a-40f0-8eeb-c24924592684",
   "metadata": {},
   "source": [
    "Full Name:\n",
    "\n",
    "Username:\n",
    "\n",
    "ID:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165d7d8",
   "metadata": {},
   "source": [
    "### 1.1.1 Create SparkSession<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Write the code to get a SparkSession. For creating the SparkSession, you need to use a SparkConf object to configure the Spark app with a proper application name, to enable the maximum partition size not exceed 10MB, and to run locally with as many working processors as local cores on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79893150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111a90",
   "metadata": {},
   "source": [
    "### 1.1.2 Define schemas<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Write code to define the data schema for features, sales and stores datasets, following the data types suggested in the metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e8dc6ab-da3f-4449-97aa-dc1a49881bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e9922b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f393574",
   "metadata": {},
   "source": [
    "### 1.1.3 Load three DF<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Using predefined schema, write code to load the features, sales and stores csv files into separate dataframes. Print the schemas for all of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36886161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb330511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cf57c51",
   "metadata": {},
   "source": [
    "### 1.2.1 Exploring the data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Write code to show the total ‘null’ counts for each column in all of the three dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e41fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2466cb0",
   "metadata": {},
   "source": [
    "### 1.2.2 Exploring the data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "For each features, sales and stores dataframe, write code to show the basic statistics (including count, mean, stddev, min, max, 25 percentile, 50 percentile, 75 percentile) for each numeric column. For each non-numeric feature in each dataframe, display the top-5 values and the corresponding counts, except for the columns of “Store”, “Dept\" and \"Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203fed84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a428671c",
   "metadata": {},
   "source": [
    "### 1.2.3 Exploring the data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Write code to display a histogram to show the distribution of the weekly sales for stores with log-scale for the frequency axis. Describe what you observe from the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93e713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65fb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f829dc4",
   "metadata": {},
   "source": [
    "### 1.2.3 Exploring the data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Apart from that, Draw a line-plot to show the trend of the average weekly sales of the month based on the different stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d87f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8f39ee1",
   "metadata": {},
   "source": [
    "### 1.2.4 Exploring the data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Explore the data provided and write code to present two plots totally worthy of presenting to the MelbourneGig company, describe your plots and discuss the findings from the plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2eadc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b71fa40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a00f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e186926c",
   "metadata": {},
   "source": [
    "### 2.1.1 Discuss the feature selection and prepare the feature columns<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "As we need to perform a one-step time-series prediction, meaning that the model’s prediction for the next weekly sales would be based on the previous weekly sales. The model will be used for future prediction Based on the data exploration from 1.2 and consider the situation we have, discuss which importances of those features (For example,which feature maybe useless and should be removed, which feature have a great impact to the label column,which features should be transformed) which features you are planning to use? Discuss the reasons for selecting them and how you create/transform them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b9748",
   "metadata": {},
   "source": [
    "Answer : Put in Documentation Part - Not here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec25d5",
   "metadata": {},
   "source": [
    "### 2.1.2 Discuss the feature selection and prepare the feature columns<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Write code to create the columns based on your discussion above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8a734",
   "metadata": {},
   "source": [
    "Use case 1: We need a model to predict stores which achieve the goals, which means the weekly sales of the store divide the Store size is greater than 8.5, create a column called \"achieve_goal\" and use 1 to label those achieved data, 0 for others’ data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa97f6a",
   "metadata": {},
   "source": [
    "Use case 2: Join the DataFrames for our one-step time-series weekly sales prediction for stores. You should make sure the weekly sales of the previous week and Store Type as the feature of our model. For other columns, you can choose based on your answer in 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2fd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a6c0ca7",
   "metadata": {},
   "source": [
    "### 2.2.1 Preparing Spark ML Transformers/Estimators for features, labels and models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Write code to create Transformers/Estimators for transforming/assembling the columns you selected above in 2.1, and create ML model Estimators for Decision Tree and Gradient Boosted Tree model for each use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4369e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ec46c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53aa715b",
   "metadata": {},
   "source": [
    "### 2.2.2 Preparing Spark ML Transformers/Estimators for features, labels and models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Write code to include the above Transformers/Estimators into pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e00a16a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3beeef1e",
   "metadata": {},
   "source": [
    "### 2.3.1 Preparing the training data and testing data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Write code to split the data for training and testing purpose - use the data in 2010 and 2012 for training purpose and the half data in 2011 year for training and others as testing purpose; then cache the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815611d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "618c7ec8",
   "metadata": {},
   "source": [
    "### 2.4.1 Training and evaluating models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "For use case 1, write code to use the corresponding ML Pipelines to train the models on the training data from 2.3. And then use the trained models to perform predictions on the testing data from 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7bd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdcb9889",
   "metadata": {},
   "source": [
    "### 2.4.2 Training and evaluating models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "For both models’ results in use case 1, write code to display the count of each combination of above-threshold/below-threshold label and prediction label in formats like the screenshot below. Compute the AUC, accuracy, recall and precision for the above-threshold/below-threshold label from each model testing result using pyspark MLlib/ML APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56fcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1acfe38b",
   "metadata": {},
   "source": [
    "### 2.4.2 Training and evaluating models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Draw a ROC plot for any model you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edff8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18182a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c57198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97f88795",
   "metadata": {},
   "source": [
    "### 2.4.2 Training and evaluating models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Discuss which is the better model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae55c66",
   "metadata": {},
   "source": [
    "Discussion In the Document Part "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe4f3a",
   "metadata": {},
   "source": [
    "### 2.4.3 Training and evaluating models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "For use case 2, write code to use the corresponding ML Pipelines to train the models on the cache training data from 2.3. And then use the trained models to perform predictions on the testing data from 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d08a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78143600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "252f9fcd",
   "metadata": {},
   "source": [
    "### 2.4.4 Evaluate regression models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Persist the better model you selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ebce3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77691124",
   "metadata": {},
   "source": [
    "### 2.4.5 Evaluate regression models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Write code to print out the features with each corresponding feature importance for the GBT model, ranked the result based on feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c485475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ab8e3af",
   "metadata": {},
   "source": [
    "### 3.1 Knowledge sharing\n",
    "How many jobs are observed when training the KMeans clustering model following the code below? Provide a screenshot from Spark UI for running a simple KMeans model training from the provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a04e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05c643c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ae4cd7",
   "metadata": {},
   "source": [
    "### 3.2 Explaining Kmeans\n",
    "Combining the parallelism theory from lecture, Spark source code, and the Spark UI, discuss which parallelism of the kmeans algorithm in spark is more likely to belong to, data parallelism or result parallelism?\n",
    "- 300 words max for the discussion\n",
    "- Hint - you can also refer to the Spark source code on github https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/clustering/KMeans.scala\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b77d3",
   "metadata": {},
   "source": [
    "Answer: In the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01a3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
