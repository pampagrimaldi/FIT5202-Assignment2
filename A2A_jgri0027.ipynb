{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adda5bfa-1f9a-40f0-8eeb-c24924592684",
   "metadata": {},
   "source": [
    "Full Name: Juan Pablo Grimaldi\n",
    "\n",
    "Username: jgri0027\n",
    "\n",
    "ID: 32980523\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165d7d8",
   "metadata": {},
   "source": [
    "### 1.1.1 Create SparkSession<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Write the code to get a SparkSession. For creating the SparkSession, you need to use a SparkConf object to configure the Spark app with a proper application name, to enable the maximum partition size not exceed 10MB, and to run locally with as many working processors as local cores on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7c03588-aaee-412f-9c11-e37aecf1b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession  # Spark SQL\n",
    "\n",
    "# the below setup will run Spark in local mode with * working processors (equal to logical cores on the machine)\n",
    "master = \"local[*]\"\n",
    "\n",
    "# Setup `appName` field to be displayed at Spark cluster UI page\n",
    "app_name = \"FIT5202 Assignment 2\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = (SparkConf()\n",
    "              .setMaster(master)\n",
    "              .setAppName(app_name)\n",
    "              .set('spark.sql.files.maxPartitionBytes', '10mb'))\n",
    "\n",
    "# Setup SparkSession and configure it with Melbourne timezone.\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.config(conf=spark_conf)\n",
    "    .config(\"spark.sql.session.timeZone\", \"UTC+11\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Duser.timezone=GMT+11\")\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Duser.timezone=GMT+11\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbebdaf-4239-40b9-9027-59d5b455c812",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\"> Documentation: </strong>\n",
    "Explain the SparkConf object that you have created, and how do you set the enable the maximum partition (1 mark).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00497d96-be92-4ec5-86f6-bdabe2ca81df",
   "metadata": {},
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "792bbd15-5a1b-45b2-bd88-ceee2b938fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_display(df, name):\n",
    "    \"\"\"\n",
    "    print schema of a Spark Dataframe\n",
    "    df: Spark Dataframe object\n",
    "    name: Given name to the object\n",
    "    \"\"\"\n",
    "    print(\"---- {} ----\".format(name))\n",
    "    pp.pprint(df.printSchema())\n",
    "\n",
    "\n",
    "def check_nulls(df, name):\n",
    "    \"\"\"\n",
    "    Check for missing data and null values in Spark Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"---- {} Missing/Null values ----\".format(name))\n",
    "    df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c)\n",
    "               for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288d38e-3189-44e7-ac19-20d1a3e22b77",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\"> Documentation: </strong>\n",
    "Explain each UDF (and add more if necesary)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d57f7049-43b8-43f1-a81c-bfc0ec64fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: What about Melbourne time zone?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11111a90",
   "metadata": {},
   "source": [
    "### 1.1.2 Define schemas<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Write code to define the data schema for features, sales and stores datasets, following the data types suggested in the metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4e8dc6ab-da3f-4449-97aa-dc1a49881bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import pprint as pp\n",
    "import matplotlib.pyplot as plt\n",
    "# Import PySpark Pandas\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, to_date, to_timestamp, isnan, when, count\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5e9922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign data type for each dataframe column\n",
    "# features data type\n",
    "features_labels = [\n",
    "    (\"Store\", StringType()),\n",
    "    (\"Date\", StringType()),\n",
    "    (\"Temperature\", FloatType()),\n",
    "    (\"Fuel_Price\", FloatType()),\n",
    "    (\"MarkDown1\", FloatType()),\n",
    "    (\"MarkDown2\", FloatType()),\n",
    "    (\"MarkDown3\", FloatType()),\n",
    "    (\"MarkDown4\", FloatType()),\n",
    "    (\"MarkDown5\", FloatType()),\n",
    "    (\"CPI\", FloatType()),\n",
    "    (\"Unemployment\", FloatType()),\n",
    "    (\"IsHoliday\", StringType()),\n",
    "]\n",
    "# sales data types\n",
    "sales_labels = [\n",
    "    (\"Store\", StringType()),\n",
    "    (\"Dept\", StringType()),\n",
    "    (\"Date\", StringType()),\n",
    "    (\"Weekly_Sales\", FloatType()),\n",
    "    (\"IsHoliday\", StringType()),\n",
    "]\n",
    "# stores data type\n",
    "stores_labels = [\n",
    "    (\"Store\", StringType()),\n",
    "    (\"Type\", StringType()),\n",
    "    (\"Size\", IntegerType()),\n",
    "]\n",
    "\n",
    "# features schema\n",
    "features_schema = StructType(\n",
    "    [StructField(x[0], x[1], True) for x in features_labels])\n",
    "# sales schema\n",
    "sales_schema = StructType([StructField(x[0], x[1], True)\n",
    "                          for x in sales_labels])\n",
    "# stores schema\n",
    "stores_schema = StructType([StructField(x[0], x[1], True)\n",
    "                           for x in stores_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ec16f-a3ee-4706-9ef4-ea919f8276bd",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\"> Documentation: </strong>\n",
    "Explain the schema of each DataFrame you have created (e.g., 1. spark objects do you use. 2. parameters) (2 marks)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f393574",
   "metadata": {},
   "source": [
    "### 1.1.3 Load three DF<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Using predefined schema, write code to load the features, sales and stores csv files into separate dataframes. Print the schemas for all of the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36886161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read with utf-8 encoding.\n",
    "\n",
    "df_features = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"encoding\", \"UTF-8\")\n",
    "    .load(\"data/Features.csv\", schema=features_schema)\n",
    ")\n",
    "\n",
    "df_sales = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"encoding\", \"UTF-8\")\n",
    "    .load(\"data/sales.csv\", schema=sales_schema)\n",
    ")\n",
    "\n",
    "df_stores = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"encoding\", \"UTF-8\")\n",
    "    .load(\"data/stores.csv\", schema=stores_schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fb330511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Features DF Schema ----\n",
      "root\n",
      " |-- Store: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Temperature: float (nullable = true)\n",
      " |-- Fuel_Price: float (nullable = true)\n",
      " |-- MarkDown1: float (nullable = true)\n",
      " |-- MarkDown2: float (nullable = true)\n",
      " |-- MarkDown3: float (nullable = true)\n",
      " |-- MarkDown4: float (nullable = true)\n",
      " |-- MarkDown5: float (nullable = true)\n",
      " |-- CPI: float (nullable = true)\n",
      " |-- Unemployment: float (nullable = true)\n",
      " |-- IsHoliday: string (nullable = true)\n",
      "\n",
      "None\n",
      "---- Sales DF Schema ----\n",
      "root\n",
      " |-- Store: string (nullable = true)\n",
      " |-- Dept: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Weekly_Sales: float (nullable = true)\n",
      " |-- IsHoliday: string (nullable = true)\n",
      "\n",
      "None\n",
      "---- Stores DF Schema ----\n",
      "root\n",
      " |-- Store: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "schema_display(df_features, \"Features DF Schema\")\n",
    "schema_display(df_sales, \"Sales DF Schema\")\n",
    "schema_display(df_stores, \"Stores DF Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b16f6-1da4-4913-9e74-e16fdf451982",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\"> Documentation: </strong>\n",
    "Explain the spark object you used to load the data into one of a dataframe (e.g., 1. spark objects do you use. 2. parameters) (1 mark)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf57c51",
   "metadata": {},
   "source": [
    "### 1.2.1 Exploring the data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Write code to show the total ‘null’ counts for each column in all of the three dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12e41fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Features Missing/Null values ----\n",
      "+-----+----+-----------+----------+---------+---------+---------+---------+---------+---+------------+---------+\n",
      "|Store|Date|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|CPI|Unemployment|IsHoliday|\n",
      "+-----+----+-----------+----------+---------+---------+---------+---------+---------+---+------------+---------+\n",
      "|    0|   0|          0|         0|     4158|     5269|     4577|     4726|     4140|585|         585|        0|\n",
      "+-----+----+-----------+----------+---------+---------+---------+---------+---------+---+------------+---------+\n",
      "\n",
      "---- Sales Missing/Null values ----\n",
      "+-----+----+----+------------+---------+\n",
      "|Store|Dept|Date|Weekly_Sales|IsHoliday|\n",
      "+-----+----+----+------------+---------+\n",
      "|    0|   0|   0|           0|        0|\n",
      "+-----+----+----+------------+---------+\n",
      "\n",
      "---- Stores Missing/Null values ----\n",
      "+-----+----+----+\n",
      "|Store|Type|Size|\n",
      "+-----+----+----+\n",
      "|    0|   0|   0|\n",
      "+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_nulls(df_features, \"Features\")\n",
    "check_nulls(df_sales, \"Sales\")\n",
    "check_nulls(df_stores, \"Stores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d236a5-5185-47c8-bd75-9c80abc047d5",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\"> Documentation: </strong>\n",
    "Explain the process in this task (e.g., the steps in a function you have created). Any method of a pyspark object used, the parameters, and the variables involved (1 mark)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2466cb0",
   "metadata": {},
   "source": [
    "### 1.2.2 Exploring the data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "For each features, sales and stores dataframe, write code to show the basic statistics (including count, mean, stddev, min, max, 25 percentile, 50 percentile, 75 percentile) for each numeric column. For each non-numeric feature in each dataframe, display the top-5 values and the corresponding counts, except for the columns of “Store”, “Dept\" and \"Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "203fed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF\n",
    "def basic_statistics(df,name):\n",
    "    \"\"\"\n",
    "    Basic statistics for numeric and non-numeric columns in a given dataframe.\n",
    "    The function splits columns into numeric and non-numeric\n",
    "    \"\"\"\n",
    "    new_df = ps.DataFrame(df)\n",
    "    num_cols = []\n",
    "    nonnum_cols = []\n",
    "    # split columns into numeric and non-numeric\n",
    "    for each in df.dtypes:\n",
    "        if each[1] == 'string' and each[0] not in ('Store','Dept','Date'):\n",
    "            nonnum_cols.append(each[0])\n",
    "        else:\n",
    "            num_cols.append(each[0])\n",
    "    # numeric column summary\n",
    "    print(f\"---- {name} Dataframe Numeric Statistical Summary ----\\n\")\n",
    "    pp.pprint(new_df.describe())\n",
    "    print(\"\\n\")\n",
    "    # non-numeric column summary\n",
    "    print(f\"---- {name} Non-Numeric Statistical Summary ----\\n\")\n",
    "    pp.pprint(new_df[nonnum_cols].describe())\n",
    "    #filter out Store, Dept and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "58724e07-1482-44ae-bd3a-2d1e53085279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Features Dataframe Numeric Statistical Summary ----\n",
      "\n",
      "       Temperature   Fuel_Price      MarkDown1      MarkDown2      MarkDown3     MarkDown4      MarkDown5          CPI  Unemployment\n",
      "count  8190.000000  8190.000000    4032.000000    2921.000000    3613.000000   3464.000000    4050.000000  7605.000000   7605.000000\n",
      "mean     59.356198     3.405992    7032.371786    3384.176593    1760.100176   3292.935892    4132.216427   172.460809      7.826821\n",
      "std      18.678607     0.431337    9262.747449    8793.583001   11276.462187   6792.329889   13086.690637    39.738346      1.877259\n",
      "min      -7.290000     2.472000   -2781.450000    -265.760000    -179.260000      0.220000    -185.170000   126.064000      3.684000\n",
      "25%      45.900000     3.041000    1577.330000      68.880000       6.600000    303.930000    1440.800000   132.364840      6.634000\n",
      "50%      60.710000     3.512000    4737.510000     364.570000      36.260000   1175.760000    2725.280000   182.716840      7.806000\n",
      "75%      73.880000     3.743000    8922.920000    2153.350000     163.150000   3309.980000    4832.680000   213.932420      8.567000\n",
      "max     101.950000     4.468000  103184.980000  104519.540000  149483.310000  67474.850000  771448.100000   228.976460     14.313000\n",
      "\n",
      "\n",
      "---- Features Non-Numeric Statistical Summary ----\n",
      "\n",
      "       IsHoliday\n",
      "count       8190\n",
      "unique         2\n",
      "top        FALSE\n",
      "freq        7605\n"
     ]
    }
   ],
   "source": [
    "basic_statistics(df_features,\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "20308906-97b6-4058-9224-2c8d09955612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1778    4.468\n",
       "2142    4.468\n",
       "5054    4.468\n",
       "5964    4.468\n",
       "6874    4.468\n",
       "Name: Fuel_Price, dtype: float32"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.DataFrame(df_features)['Fuel_Price'].nlargest(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "095341a5-bf6c-4eb0-af7b-78c490b2fc04",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Sales Dataframe Numeric Statistical Summary ----\n",
      "\n",
      "        Weekly_Sales\n",
      "count  421570.000000\n",
      "mean    15981.258121\n",
      "std     22711.183512\n",
      "min     -4988.940000\n",
      "25%      2079.390000\n",
      "50%      7612.100000\n",
      "75%     20200.750000\n",
      "max    693099.400000\n",
      "\n",
      "\n",
      "---- Sales Non-Numeric Statistical Summary ----\n",
      "\n",
      "       IsHoliday\n",
      "count     421570\n",
      "unique         2\n",
      "top        FALSE\n",
      "freq      391909\n"
     ]
    }
   ],
   "source": [
    "basic_statistics(df_sales,\"Sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c9482e78-d35f-4181-abdf-050d77e54a68",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Stores Dataframe Numeric Statistical Summary ----\n",
      "\n",
      "                Size\n",
      "count      45.000000\n",
      "mean   130287.600000\n",
      "std     63825.271991\n",
      "min     34875.000000\n",
      "25%     70713.000000\n",
      "50%    126512.000000\n",
      "75%    202307.000000\n",
      "max    219622.000000\n",
      "\n",
      "\n",
      "---- Stores Non-Numeric Statistical Summary ----\n",
      "\n",
      "       Type\n",
      "count    45\n",
      "unique    3\n",
      "top       A\n",
      "freq     22\n"
     ]
    }
   ],
   "source": [
    "basic_statistics(df_stores,\"Stores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428671c",
   "metadata": {},
   "source": [
    "### 1.2.3 Exploring the data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Write code to display a histogram to show the distribution of the weekly sales for stores with log-scale for the frequency axis. Describe what you observe from the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93e713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab65fb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f829dc4",
   "metadata": {},
   "source": [
    "### 1.2.3 Exploring the data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Apart from that, Draw a line-plot to show the trend of the average weekly sales of the month based on the different stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d87f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8f39ee1",
   "metadata": {},
   "source": [
    "### 1.2.4 Exploring the data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "Explore the data provided and write code to present two plots totally worthy of presenting to the MelbourneGig company, describe your plots and discuss the findings from the plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2eadc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b71fa40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a00f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e186926c",
   "metadata": {},
   "source": [
    "### 2.1.1 Discuss the feature selection and prepare the feature columns<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "\n",
    "As we need to perform a one-step time-series prediction, meaning that the model’s prediction for the next weekly sales would be based on the previous weekly sales. The model will be used for future prediction Based on the data exploration from 1.2 and consider the situation we have, discuss which importances of those features (For example,which feature maybe useless and should be removed, which feature have a great impact to the label column,which features should be transformed) which features you are planning to use? Discuss the reasons for selecting them and how you create/transform them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b9748",
   "metadata": {},
   "source": [
    "Answer : Put in Documentation Part - Not here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec25d5",
   "metadata": {},
   "source": [
    "### 2.1.2 Discuss the feature selection and prepare the feature columns<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Write code to create the columns based on your discussion above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8a734",
   "metadata": {},
   "source": [
    "Use case 1: We need a model to predict stores which achieve the goals, which means the weekly sales of the store divide the Store size is greater than 8.5, create a column called \"achieve_goal\" and use 1 to label those achieved data, 0 for others’ data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa97f6a",
   "metadata": {},
   "source": [
    "Use case 2: Join the DataFrames for our one-step time-series weekly sales prediction for stores. You should make sure the weekly sales of the previous week and Store Type as the feature of our model. For other columns, you can choose based on your answer in 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2fd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a6c0ca7",
   "metadata": {},
   "source": [
    "### 2.2.1 Preparing Spark ML Transformers/Estimators for features, labels and models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Write code to create Transformers/Estimators for transforming/assembling the columns you selected above in 2.1, and create ML model Estimators for Decision Tree and Gradient Boosted Tree model for each use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4369e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ec46c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53aa715b",
   "metadata": {},
   "source": [
    "### 2.2.2 Preparing Spark ML Transformers/Estimators for features, labels and models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Write code to include the above Transformers/Estimators into pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e00a16a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3beeef1e",
   "metadata": {},
   "source": [
    "### 2.3.1 Preparing the training data and testing data<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Write code to split the data for training and testing purpose - use the data in 2010 and 2012 for training purpose and the half data in 2011 year for training and others as testing purpose; then cache the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815611d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "618c7ec8",
   "metadata": {},
   "source": [
    "### 2.4.1 Training and evaluating models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "For use case 1, write code to use the corresponding ML Pipelines to train the models on the training data from 2.3. And then use the trained models to perform predictions on the testing data from 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7bd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdcb9889",
   "metadata": {},
   "source": [
    "### 2.4.2 Training and evaluating models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "For both models’ results in use case 1, write code to display the count of each combination of above-threshold/below-threshold label and prediction label in formats like the screenshot below. Compute the AUC, accuracy, recall and precision for the above-threshold/below-threshold label from each model testing result using pyspark MLlib/ML APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56fcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1acfe38b",
   "metadata": {},
   "source": [
    "### 2.4.2 Training and evaluating models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Draw a ROC plot for any model you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edff8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18182a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c57198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97f88795",
   "metadata": {},
   "source": [
    "### 2.4.2 Training and evaluating models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Discuss which is the better model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae55c66",
   "metadata": {},
   "source": [
    "Discussion In the Document Part "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe4f3a",
   "metadata": {},
   "source": [
    "### 2.4.3 Training and evaluating models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "For use case 2, write code to use the corresponding ML Pipelines to train the models on the cache training data from 2.3. And then use the trained models to perform predictions on the testing data from 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d08a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78143600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "252f9fcd",
   "metadata": {},
   "source": [
    "### 2.4.4 Evaluate regression models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Persist the better model you selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ebce3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77691124",
   "metadata": {},
   "source": [
    "### 2.4.5 Evaluate regression models<a class=\"anchor\" id=\"1.1.1\"></a>\n",
    "[Back to top](#table)\n",
    "Write code to print out the features with each corresponding feature importance for the GBT model, ranked the result based on feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c485475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ab8e3af",
   "metadata": {},
   "source": [
    "### 3.1 Knowledge sharing\n",
    "How many jobs are observed when training the KMeans clustering model following the code below? Provide a screenshot from Spark UI for running a simple KMeans model training from the provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a04e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05c643c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ae4cd7",
   "metadata": {},
   "source": [
    "### 3.2 Explaining Kmeans\n",
    "Combining the parallelism theory from lecture, Spark source code, and the Spark UI, discuss which parallelism of the kmeans algorithm in spark is more likely to belong to, data parallelism or result parallelism?\n",
    "- 300 words max for the discussion\n",
    "- Hint - you can also refer to the Spark source code on github https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/clustering/KMeans.scala\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b77d3",
   "metadata": {},
   "source": [
    "Answer: In the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01a3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
